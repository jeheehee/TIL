# 단어임베딩

**옷 = 0, 의류 = 1, 의복 = 2** 처럼 임의의 번호. ==> **옷 = (0.1, 0.1), 의류 = (0.2, 0.1), 의복 = (0.1, -0.1)** 벡터값을 대응 (**실질적인 의미를 반영하게 됨**)

 0, 1, 2번에는 아무런 의미가 없고 단지 계산을 편리하게 하기 위해 정수를 붙인것 뿐임. 이를 통해 간편하게 0번이 몇번 등장하는지, 1번이 몇번 등장하는지 등을 셀 수 있고 이 빈도가 전체 문헌의 의미를 나타낸다고 가정하여 분석을 진행했었음.

하지만 **단어 임베딩 기법**에서는 각 단어를 임의의 차원의 실수로 대응시킴.이 때, 각 벡터값(2개의 실수를 묶어서 사용)은 실질적인 의미를 반영함. 의미가 유사한 단어가 실제로 유사한 벡터값을 가지게 한다는 것. 이를 수행하는 대표적인 기법: (Word2Vec,  GloVe, FastText).   이전에는 {옷, 의류, 의복}이라는 단어 사이의 관계를 전혀 반영하지 않은 채로 분석을 진행했다면, 단어 임베딩 기법이 등장하면서 이 단어들이 유사한 단어라는 것을 알고리즘 상에 반영하여 분석을 진행할 수 있는 여지가 생김.